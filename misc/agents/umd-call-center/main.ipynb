{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env.local\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .docx to text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AzureAI Document Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders.doc_intelligence import (\n",
    "    AzureAIDocumentIntelligenceLoader,\n",
    ")\n",
    "import os\n",
    "AZURE_DOCUMENT_INTELLIGENCE_API_KEY = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_API_KEY\")\n",
    "def load_docs_adi(filepath: str):\n",
    "    adi = AzureAIDocumentIntelligenceLoader(\n",
    "        \"https://eastus.api.cognitive.microsoft.com\",\n",
    "        AZURE_DOCUMENT_INTELLIGENCE_API_KEY,\n",
    "        file_path=filepath,\n",
    "        api_model=\"prebuilt-read\",\n",
    "    )\n",
    "    docs = adi.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_docs_pypdf(file_path: str):\n",
    "    pypdf_loader = PyPDFLoader(\n",
    "        file_path = file_path,\n",
    "        password = None,\n",
    "        extract_images = True,\n",
    "        # headers = None\n",
    "        # extraction_mode = \"plain\",\n",
    "        # extraction_kwargs = None,\n",
    "    )\n",
    "    docs = pypdf_loader.load()\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zerox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "def load_docs_zerox(path:str):\n",
    "    with open(path, \"r\") as f:\n",
    "        data = f.read()\n",
    "    return [Document(page_content=data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "#file_path = os.path.join(current_dir, \"RA Duty Manual - Final Draft 2023-2024.pdf\")\n",
    "file_path = os.path.join(current_dir, \"./zerox/output.md\")\n",
    "\n",
    "docs = load_docs_zerox(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone.grpc import GRPCVector\n",
    "from pinecone.grpc.index_grpc import UpsertResponse\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_HOST = os.getenv(\"PINECONE_HOST\")\n",
    "PINECONE_NAMESPACE = \"umd-call-center\"\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "PC_INDEX_NAME = \"knowledge\"\n",
    "pc_index = pc.Index(PC_INDEX_NAME, host=PINECONE_HOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def query_pc(vector: List[float]):\n",
    "    query_result = pc_index.query(\n",
    "        vector=vector,\n",
    "        namespace=PINECONE_NAMESPACE,\n",
    "        top_k=10,\n",
    "        #filter={\"knowledge_uuid\": {\"$in\": knowledge_uuids}},\n",
    "        include_metadata=True,\n",
    "        timeout=1,\n",
    "    )\n",
    "    return query_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Local Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "embedder = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\", api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "\n",
    "def split_by_semantics(content:str):\n",
    "    semantic_splitter = SemanticChunker(\n",
    "        embedder,\n",
    "        add_start_index=True,\n",
    "        sentence_split_regex=r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)(?=\\s)\",\n",
    "    )\n",
    "    splits = semantic_splitter.split_documents([Document(page_content=content)])\n",
    "    start_index = 0\n",
    "    for i, split in enumerate(splits):\n",
    "        last_index = start_index + len(content) - 1\n",
    "        split.metadata[\"start_index\"] = start_index\n",
    "        start_index = last_index + 1\n",
    "    return splits\n",
    "\n",
    "def split_by_character(content:str, chunk_size:int=800, overlap:int=400):\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=overlap, add_start_index=True\n",
    "    )\n",
    "    splits = recursive_splitter.split_documents([Document(page_content=content)])\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval relevence\n",
    "Are the documents that were retrieved relevent to the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Faithfulness\n",
    "Is the answer grounded in the documents?\n",
    "- This can measure hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Correctness\n",
    "Is the answer consistent with a reference answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gen dataset giskard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "splits = []\n",
    "chunk_size = 800\n",
    "chunk_overlap = 0\n",
    "for doc in docs:\n",
    "    splits.extend(split_by_character(doc.page_content, chunk_size=chunk_size, overlap=overlap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0b8fe8af-e795-482a-8186-f96ace3bc29b',\n",
       " 'a021b58f-269b-448f-a9f4-2d1e228c3f83',\n",
       " 'a9098cc2-c426-48c5-a347-362ef40e386f',\n",
       " '49fa7c4b-5ec2-4eea-824a-7bbe4493080f',\n",
       " 'c5a9e988-1f48-411f-8302-5ad8c183c14a',\n",
       " '7bdbd29c-da1d-4161-a57e-ba483c5975fe',\n",
       " '2d8fddb2-ad13-4ded-80a8-d53bf9e79b10',\n",
       " '9eb306e3-62ff-4baa-ab6d-78fb0ff5a46c',\n",
       " '35891cd3-a1ec-4f1a-9262-0872b7b2204b',\n",
       " '8974aa7e-8070-4efe-8922-38349010e279',\n",
       " '04aa3f91-b8a8-4f51-8e4d-83e86f5f69d8',\n",
       " '2d7e7aac-410e-43f6-bdb2-a02f5f38a620',\n",
       " 'fdd429a0-f69c-4abc-8a1b-213fe5f09346',\n",
       " 'a426fc5f-f818-47dc-afc5-6a1a8c9f64b3',\n",
       " 'd2d829f7-dbc3-43b4-bec6-3eb6214bd0d0',\n",
       " '216ebf1a-97cc-4013-9405-7f625e6af91d',\n",
       " '741d2fd3-1723-4700-a857-f049888084a3',\n",
       " '46d9db90-6e85-4aed-9afc-9d8690122e0f',\n",
       " '4ee49b6a-2eaf-4ca0-9349-e93033b1592e',\n",
       " '3b09d263-6d5d-4d95-8246-6c522c495698',\n",
       " '75abdffa-738e-4e21-b7d6-ed0f57f82f8f',\n",
       " 'f77c9528-7abd-4280-8546-45243e38a5da',\n",
       " '2f22285e-11c7-457d-8f37-2096c9145baf',\n",
       " 'be39b362-d315-4e40-b90b-3ef0b7d3eb6e',\n",
       " '9d966b5c-4f6e-4c26-82c1-2dafa75fbd50',\n",
       " '8016e4ca-166c-4dc2-9ffe-690cab8b3a77',\n",
       " 'fa1311ca-0b9c-431c-8028-9f0dc81a0fe3',\n",
       " '82f75c6f-aa54-4e0b-a159-1adf8bce64b8',\n",
       " 'f5ff65a7-ae96-4b64-964a-483734adb686',\n",
       " '49fb9aa0-0311-44ea-be88-d34702030242',\n",
       " '5494fe24-07a7-4ccd-96a2-575129f84a41',\n",
       " '6417d4e5-e74b-463f-9906-c29d55957335',\n",
       " '92ef947a-ca72-444c-8c7a-a35fa47696e2',\n",
       " 'dc3a3923-3e18-467a-a798-7232f31cda7a',\n",
       " 'a277c39b-1ed6-40a2-a183-4a242c1b639c',\n",
       " 'dac64087-2f96-4bd1-8b43-0a56f1f6a177',\n",
       " 'b2510cbf-0831-40c9-9263-73f760325ae6',\n",
       " 'ed419fda-b3d4-4193-b6a3-ab8a7f8417e2',\n",
       " 'ca84757b-0ae0-4fd0-8154-089d611944fe',\n",
       " '7b40be8f-1d23-42eb-802d-77f82e2f7efa',\n",
       " '98b3c5e2-5fa7-4981-9003-1b19128421ed',\n",
       " 'a3d5a3ef-f8a7-4deb-bff4-4f33093dec78',\n",
       " '87eef5e2-6b8a-4b71-bb0a-29afe7db8a40',\n",
       " '39decd38-2fe7-4f7c-a060-55394aab3e2e',\n",
       " '1d1c5890-9b18-4a7a-8994-351b691a6a5c',\n",
       " '44373114-b383-4634-8ac0-b4d77138d8ec',\n",
       " '5458f976-3079-40be-a3a4-ae08abb22b58',\n",
       " '9f4275ac-35d7-4e98-943c-aa58b46afb9f',\n",
       " '11be1996-43a8-4127-b15f-b6dba28309f0',\n",
       " '9dfa0910-e8d7-48de-8330-abf17d4f2dd2',\n",
       " 'ce754e57-fb99-42a0-8171-3e964085e058',\n",
       " '529e4cc8-50ff-4253-a134-edc989cd6b15',\n",
       " '2417c73d-2546-4198-a4a3-72495a59e832',\n",
       " 'c0069603-dc37-4b71-8c1d-00ae3efb1f59',\n",
       " 'ef5ad80b-f10d-4363-a7cb-5458a34273c3',\n",
       " '6a9335df-9959-4a14-8b34-b225a0773304',\n",
       " '1e153b30-350e-4f24-a753-c4489c944584',\n",
       " 'b2e3928b-a980-4e65-83d1-233df1cf75f1',\n",
       " '84d0373c-a9c2-48f3-b904-26edf2457d6b',\n",
       " '794e640e-a128-4a03-a21a-e291c7f2e113',\n",
       " '746998c9-3ce5-4525-ac20-14e95d526519',\n",
       " 'd2902fe1-b0b1-4302-b761-5c9ea7a007b3',\n",
       " 'a57fdcce-6d75-4591-b86f-d59f62e16aed',\n",
       " 'a6512e3c-442a-476e-951c-cbb2d70b91d8',\n",
       " 'f17e27d5-062b-46d8-b63a-b923b17732ee',\n",
       " 'abb472c7-cb67-442f-ac24-d577f59131cc',\n",
       " '1243166c-b1fa-4ba4-be5c-61f075ea79f0',\n",
       " '92b87592-8280-43d2-9c58-6d3dc03fa6f9',\n",
       " '0a7bc459-22e8-4d61-b740-c6e30a2f6055',\n",
       " '871fb595-6c9c-450a-b340-9c4de420c000',\n",
       " '26927148-40ef-4d83-8e0f-3da436d1061d',\n",
       " 'a2d86ef7-89cc-4ce9-aaba-6e55af19dbee',\n",
       " 'b0b3181b-98ba-479b-8ced-d9b224224b3a',\n",
       " '761306d6-756d-4848-ac15-f9ee15b54087',\n",
       " '5e17c88d-8f09-43ee-a176-c5a0a9f4d5c1',\n",
       " 'c9eb7ec4-d920-4122-990c-cbbfad7d8ee5',\n",
       " 'cc7cc154-04dc-44a6-b1be-2d533126a2a0',\n",
       " '95ce4cc4-288b-4dd3-96fd-81b6c7ced528',\n",
       " '4d27dc86-439f-45e1-8cb3-a88f39eb5e6d',\n",
       " 'f266bf89-fbfb-49ca-88d3-8543adc6e026',\n",
       " 'a6c39bd1-7bc2-4380-a723-5a8c5fb05d28',\n",
       " 'b403f57c-9c13-4f02-8449-22d9483ce206',\n",
       " 'e825d2c6-d422-4019-b16a-1650befa284b',\n",
       " 'effe2149-8842-44d9-8a4e-eaa64ae9ce6e',\n",
       " 'b6cba90b-fca0-4b66-a654-46c8e9811f23',\n",
       " 'aefc9076-2ea0-4d0c-b584-a0a43902c10d',\n",
       " 'a5f7a685-4898-4e44-a1f4-b9568f4fd86b',\n",
       " '71c28fd7-912b-48a5-92df-97ded2e1a712',\n",
       " '3360c4c4-531d-4867-acd1-6c2af9d73db8',\n",
       " 'f965d6b5-d830-4e51-b550-7b0c8bbe6b40',\n",
       " 'ce599d73-b33c-4df6-9f51-924737c4d7dc',\n",
       " '05d895dc-a7a5-41e3-926a-5b93d591ca33',\n",
       " '484584eb-ab35-463c-98ee-f52113f28112',\n",
       " 'e275c963-920f-46c8-b10b-3a4eeea2a5f0',\n",
       " '5b649dd4-969e-442b-95b2-adf391d5b371',\n",
       " 'deb01b95-0dc5-4af3-8d38-5650ad82040c',\n",
       " 'd7d2cb53-f518-4fd6-9ca8-508ac328a579',\n",
       " '133a61ce-e8a5-464f-9792-61b1c8d49ae2',\n",
       " '731e410e-baca-4f23-8a8b-6912f6cbfeec',\n",
       " 'ff0c0e51-0e5d-4eb2-a462-db0826fdee30',\n",
       " '3ff85112-57d5-4dc0-8d2c-961598be99d2',\n",
       " 'ab0f1744-5660-4d71-acee-2f3a4b488a2d',\n",
       " 'ee8795a0-299c-4518-9c43-adf1fc44257c',\n",
       " 'ea51fa41-f307-4f95-a028-fceda8f94b76',\n",
       " '2df3244c-401d-48c4-9bef-0111e8a793ac',\n",
       " 'c99ae1f2-471c-4192-9e97-7825959ec2d1',\n",
       " '8818c067-e54b-4adb-b9f5-9a18c308b729',\n",
       " 'cd9d5956-d501-4d3a-a5ec-3194491ff3cb',\n",
       " '863c8de4-afaa-4b40-a0e9-982bc598a46c',\n",
       " '4477cb4d-dacd-44fe-87de-680030375392',\n",
       " '459bf2b7-010b-4fe2-a366-79f6f09209b3',\n",
       " 'cd216e1a-582f-4166-99b4-6ac659c5583a',\n",
       " 'edbe5a95-ef90-4826-a0fd-f1539d13dc76',\n",
       " '3d1a312a-059f-4a9a-bba6-e380c90a93c6',\n",
       " 'b6992d17-bbcd-4958-af54-c9e89677301e',\n",
       " '0b826fd3-43dc-4314-9589-1027874780ec',\n",
       " '4a745920-de38-458b-8e4e-24c6a22d7467',\n",
       " 'ffd78974-5ed3-48bb-be7f-a9cfe9647a6b',\n",
       " '637d1829-3f40-447e-a3c2-2ac8c62049fd',\n",
       " '431d6bbb-f58d-4ee6-aaf4-1fcf012e0eb8',\n",
       " '8c6ffcb2-1624-4a72-841e-041da1fe4040',\n",
       " '37b45eda-7f32-4b3a-98a7-ff25e3e120b2',\n",
       " '2b6be639-4b6f-4360-a365-5b2f6d367e02',\n",
       " '1103ba22-fcba-438d-a7a2-4d501a4b875a',\n",
       " '54fc2e2a-e03e-4f7b-97d1-2d4549223de4',\n",
       " '7bf9170a-4c24-4e66-aa14-96a875c2182f',\n",
       " '6d07c7d8-e2dc-41bc-8448-7c2babd0a70f',\n",
       " 'c2c40164-11f6-4a8e-b214-2c2a4b386a65',\n",
       " '6fedbacc-0ad1-4003-9bdc-fbe7748f6369',\n",
       " '72bddfb7-428a-4e4e-a6ac-8dfae24c0ca6',\n",
       " '0121bfd7-9726-47fa-aa05-5fba72a09be6',\n",
       " 'd617003d-2dff-438e-9eef-17faee9bdeab',\n",
       " 'f9a65a75-fbd4-46a4-876c-52812e2a3eb8',\n",
       " 'f77f46eb-9680-495a-9b88-c5746f0e579d',\n",
       " '7f731ca4-ab91-46fa-9584-508ddfa1e149',\n",
       " '30478a6a-dc5f-4bac-9114-84eb163ec51b',\n",
       " '59595781-c11b-4947-b3fa-8d9e9d632972',\n",
       " '359d6a83-1f85-4fb6-878a-24dd682ac653',\n",
       " '105a36d0-844c-400d-b999-35035c71c301',\n",
       " 'de294d62-a726-4ae2-8e69-58dd61826987',\n",
       " '10f5d1f6-d4f7-4fd6-a802-11231858c3b4',\n",
       " 'a17c63d1-f481-45ea-92a3-515f4ef40cd1',\n",
       " '600e8958-11ea-4937-8971-070b46f118b2',\n",
       " 'acc4a904-4fd5-4d31-9790-b20f25c63856',\n",
       " '0d41325b-ea09-4e19-9197-87d47490edb3',\n",
       " '3b51af4f-4e3e-4cfe-9547-0fc88863d387',\n",
       " '1128119d-3f71-44ae-a8ee-b43ca4da3a8f',\n",
       " '1ed25f21-c46f-4b37-8639-aa5464857d9b',\n",
       " 'df0ebe7b-3ab5-41e6-a515-e482173f9c31',\n",
       " '73b59449-db63-4a29-b42d-1300a55cb07d',\n",
       " '627023a0-8d45-49ff-9a11-9177fcde9cf2',\n",
       " '995663f4-8d57-44b5-a0fb-918950cba275',\n",
       " '21dfc2ff-3bee-4224-8208-c1a4fdde698f',\n",
       " '6e10da6d-9564-41c2-824a-3fbc12b4bf39',\n",
       " '28c53e2d-54ac-42d7-9941-d638f61908fa',\n",
       " 'd7782e98-9984-4a8a-b310-3c2971466e10',\n",
       " '214ff2aa-ebe6-4f7c-af53-f71bc916b95b',\n",
       " 'fa243860-2823-4d43-9a8c-2797e9dea2c6',\n",
       " '0b994209-c2d8-43a5-93d3-7c84ce78ae96',\n",
       " '60f52cc8-711b-431b-8f1a-8441b0ec856c',\n",
       " '0fcdfd7f-dc44-4868-9d0f-5e562bc28012',\n",
       " 'c14e9b35-3080-43f6-852e-965a03bcc48a',\n",
       " '058a8ba3-1ce3-46e6-85e7-1c75cb23593c',\n",
       " 'a5ddb31f-12ce-4cda-91c4-7176832bf9f8',\n",
       " '49b8b1b2-75a1-4b7c-84df-72477df554b9',\n",
       " '12018f3a-850e-4c9c-8b1e-948a44187d8b',\n",
       " '428ea909-fb59-4e97-a890-790c2880cedb',\n",
       " 'dc62a87d-d75f-4852-94b4-3544b5ce32d6',\n",
       " 'dc2fd6ab-3357-4de0-8e0c-c2136e8b190d',\n",
       " '628c43a6-2a59-4ddb-be83-b553763da5e6',\n",
       " '3925fdcb-ba72-4b8f-aa52-a632edfe7b45',\n",
       " '67f92d32-1ba4-493e-87d2-89b8f407675b',\n",
       " 'c0213539-69f1-41ed-9cb6-5c1bb3b31c0a',\n",
       " '41bccb76-27ee-4383-8a8f-53d21e1654b8',\n",
       " 'ef27f0ee-096a-46e4-9b26-372222dea836',\n",
       " '2332a86b-4995-45d6-b593-e38eb266e62d',\n",
       " 'e330fb1c-241c-4254-90bf-a28c645298ce',\n",
       " '0de5de66-152d-44ee-a980-8ecc2bc00980',\n",
       " 'a4cc1a43-a50c-4448-9cfb-1348af46fabc',\n",
       " 'ae239f28-bf69-4b49-99f2-b40188a2371f',\n",
       " '547a4486-5529-488b-875b-e67c215c95a1',\n",
       " '19ca3ab9-7f28-48b4-9a37-ff1c8a1cea0b',\n",
       " '2d976b56-1b09-4993-9d29-45a42aec929a',\n",
       " '0259f6f9-f50a-46d9-b9c2-eddd8d9f460f',\n",
       " '4eac2123-4176-4e84-a8af-29a7b41641e1',\n",
       " '053ac586-ba38-4b47-9867-121e5b6dc1ed',\n",
       " '390bf27c-b798-4107-923b-17872f5a85b8',\n",
       " '09462349-2278-403a-a358-c55daa06d591',\n",
       " '1e279f63-65db-4f03-a5fb-5b19eb87c62a',\n",
       " 'f7c69ce0-b55a-4b02-ba39-20efd8e3fd10',\n",
       " '3cee0227-4604-4ec5-b08c-9583792319bd',\n",
       " 'b98ae3ac-3af8-45d4-8b99-9bb294582e90',\n",
       " '831ffd1f-9d4e-441b-b024-00b8c32d3d11',\n",
       " '57234cc5-7a71-4a51-9cbb-aea1a2fdb2bc',\n",
       " '12d4465f-b886-486d-872f-6973e7b6d3e0',\n",
       " '01c8d5f8-b5e6-44a6-b25a-72ddb6ec31f3',\n",
       " 'ba194f3c-157f-47fb-9aed-08c94f23dbad',\n",
       " '3a556a54-4293-4d0c-8f39-2102a7042ada',\n",
       " 'a508bdf9-46f4-45fe-8723-f4ba50594472',\n",
       " '4ad612a1-03bb-4aea-b30e-549cd2ed3e22',\n",
       " 'a517d8bf-8cdb-4b3f-9f22-01637bc4a87d']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store = InMemoryVectorStore(OpenAIEmbeddings(model=\"text-embedding-3-large\"))\n",
    "vector_store.add_documents(splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.dump(\"vector_store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "vector_store = InMemoryVectorStore.load(\"vector_store\", OpenAIEmbeddings(model=\"text-embedding-3-large\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from giskard.rag import KnowledgeBase\n",
    "knowledge_base_df = pd.DataFrame([i.page_content for i in splits], columns=[\"text\"])\n",
    "knowledge_base = KnowledgeBase(knowledge_base_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "from giskard.rag import RAGReport\n",
    "report = RAGReport.load(\"reports_testset250/p1-gpt_4o_mini-splits_character-max_marginal_relevance-top5-zerox-tester_gpt-4o-mini\")\n",
    "knowledge_base = report._knowledge_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import giskard\n",
    "from giskard.llm.client.openai import OpenAIClient\n",
    "\n",
    "giskard.llm.set_llm_api(\"openai\")\n",
    "oc = OpenAIClient(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o1-mini\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Optional, Sequence, Union\n",
    "from giskard.llm.client import LLMClient, ChatMessage\n",
    "import openai\n",
    "from openai.types.chat.completion_create_params import ResponseFormatJSONObject, ResponseFormatText\n",
    "import giskard\n",
    "class O1Client(LLMClient):\n",
    "    def __init__(self, model_openai = \"o1-preview\") -> None:\n",
    "        super().__init__()\n",
    "        self.model_openai = model_openai\n",
    "\n",
    "\n",
    "    def complete(self, messages: Sequence[ChatMessage], temperature: float = 1, max_tokens: Union[Optional[int], Any] = None, caller_id: str | None = None, seed: int | None = None, format=None) -> ChatMessage:\n",
    "        messages_openai = []\n",
    "        for message in messages:\n",
    "            role = message.role\n",
    "            if message.role != \"user\" or message.role != \"assistant\":\n",
    "                role = \"user\"\n",
    "            messages_openai.append({\"role\": role, \"content\": message.content})\n",
    "            \n",
    "        if format is not None and \"json\" in format:\n",
    "            rformat: ResponseFormatJSONObject = {\"type\": \"json_object\"}\n",
    "        else:\n",
    "            rformat: ResponseFormatText = {\"type\": \"text\"}\n",
    "\n",
    "        if max_tokens is None:\n",
    "            response = openai.chat.completions.create(messages=messages_openai, model=self.model_openai, temperature=1,seed=seed, response_format=rformat)\n",
    "        else:\n",
    "            response = openai.chat.completions.create(messages=messages_openai, model=self.model_openai, temperature=1, max_tokens=max_tokens, seed=seed, response_format=rformat)\n",
    "            \n",
    "        cm = ChatMessage(role=response.choices[0].message.role, content=response.choices[0].message.content)\n",
    "        return cm\n",
    "    \n",
    "o1_client= O1Client(model_openai=\"o1-mini\")\n",
    "print(o1_client.model_openai)\n",
    "giskard.llm.set_default_client(o1_client)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-29 13:18:30,547 pid:14031 MainThread giskard.rag  INFO     Finding topics in the knowledge base.\n",
      "2024-10-29 13:18:47,080 pid:14031 MainThread giskard.rag  INFO     Found 11 topics in the knowledge base.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:   0%|          | 0/55 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:   2%|▏         | 1/55 [00:06<06:04,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:   4%|▎         | 2/55 [00:12<05:34,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:   5%|▌         | 3/55 [00:17<04:58,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:   7%|▋         | 4/55 [00:22<04:30,  5.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:   9%|▉         | 5/55 [00:29<04:54,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  11%|█         | 6/55 [00:39<05:58,  7.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  13%|█▎        | 7/55 [00:47<05:59,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  15%|█▍        | 8/55 [00:55<06:05,  7.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  16%|█▋        | 9/55 [01:02<05:46,  7.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  18%|█▊        | 10/55 [01:10<05:36,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  20%|██        | 11/55 [01:24<07:00,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  22%|██▏       | 12/55 [01:36<07:30, 10.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  24%|██▎       | 13/55 [01:58<09:44, 13.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  25%|██▌       | 14/55 [02:13<09:39, 14.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  27%|██▋       | 15/55 [02:24<08:48, 13.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  29%|██▉       | 16/55 [02:38<08:43, 13.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  31%|███       | 17/55 [02:58<09:51, 15.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  33%|███▎      | 18/55 [03:16<09:58, 16.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  35%|███▍      | 19/55 [03:34<10:00, 16.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  36%|███▋      | 20/55 [03:46<08:56, 15.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  38%|███▊      | 21/55 [04:01<08:37, 15.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  40%|████      | 22/55 [04:16<08:19, 15.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  42%|████▏     | 23/55 [04:29<07:43, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  44%|████▎     | 24/55 [04:48<08:16, 16.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  45%|████▌     | 25/55 [05:02<07:35, 15.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  47%|████▋     | 26/55 [05:18<07:26, 15.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  49%|████▉     | 27/55 [05:33<07:07, 15.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  51%|█████     | 28/55 [05:49<07:01, 15.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  53%|█████▎    | 29/55 [06:05<06:52, 15.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  55%|█████▍    | 30/55 [06:26<07:12, 17.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  56%|█████▋    | 31/55 [06:46<07:16, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  58%|█████▊    | 32/55 [07:01<06:35, 17.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  60%|██████    | 33/55 [07:16<06:00, 16.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  62%|██████▏   | 34/55 [07:30<05:30, 15.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  64%|██████▎   | 35/55 [07:48<05:26, 16.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  65%|██████▌   | 36/55 [08:15<06:11, 19.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  67%|██████▋   | 37/55 [08:30<05:29, 18.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  69%|██████▉   | 38/55 [09:04<06:32, 23.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  71%|███████   | 39/55 [09:25<05:59, 22.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  73%|███████▎  | 40/55 [09:44<05:19, 21.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  75%|███████▍  | 41/55 [09:57<04:21, 18.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  76%|███████▋  | 42/55 [10:14<03:56, 18.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  78%|███████▊  | 43/55 [10:34<03:45, 18.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  80%|████████  | 44/55 [11:07<04:13, 23.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  82%|████████▏ | 45/55 [11:24<03:31, 21.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  84%|████████▎ | 46/55 [11:44<03:08, 20.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  85%|████████▌ | 47/55 [11:56<02:25, 18.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  87%|████████▋ | 48/55 [12:12<02:02, 17.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  89%|████████▉ | 49/55 [12:37<01:58, 19.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  91%|█████████ | 50/55 [12:49<01:27, 17.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  93%|█████████▎| 51/55 [13:26<01:33, 23.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  95%|█████████▍| 52/55 [14:12<01:30, 30.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  96%|█████████▋| 53/55 [14:32<00:54, 27.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions:  98%|█████████▊| 54/55 [14:59<00:27, 27.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELF  o1-mini\n",
      "SELF  o1-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating questions: 100%|██████████| 55/55 [15:15<00:00, 16.64s/it]\n"
     ]
    }
   ],
   "source": [
    "from giskard.rag import generate_testset\n",
    "\n",
    "testset = generate_testset(\n",
    "    knowledge_base,\n",
    "    num_questions=55,\n",
    "    language='en',  \n",
    "    agent_description=\"A support agent for the University of Maryland's call center.\", \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.to_pandas().to_excel(\"testset55-o1mini.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.save(\"testset45-o1preview.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from giskard.rag import QATestset\n",
    "import json\n",
    "import ast\n",
    "\n",
    "def safe_literal_eval(cell):\n",
    "    try:\n",
    "        return ast.literal_eval(cell) if isinstance(cell, str) else cell\n",
    "    except (ValueError, SyntaxError):\n",
    "        print(f\"Warning: Could not evaluate literal in cell: {cell}\")\n",
    "        return cell\n",
    "\n",
    "df = pd.read_excel(\"./testset100-o1.xlsx\")\n",
    "# Apply the functions to the specific columns after loading\n",
    "df['metadata'] = df['metadata'].apply(safe_literal_eval)\n",
    "df['conversation_history'] = df['conversation_history'].apply(safe_literal_eval)\n",
    "\n",
    "testset = QATestset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from giskard.rag import QATestset\n",
    "testset = QATestset.load(\"testset250.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate gkiskard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking questions to the agent: 100%|██████████| 100/100 [03:42<00:00,  2.23s/it]\n",
      "CorrectnessMetric evaluation: 100%|██████████| 100/100 [01:21<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal, Sequence, List\n",
    "import giskard.llm\n",
    "from giskard.llm.client.base import ChatMessage\n",
    "from giskard.rag import evaluate\n",
    "import openai\n",
    "\n",
    "prompt_type: Literal[\"api\", \"p1\", \"p2\"] = \"p2\"\n",
    "retrieval_type: Literal[\"similarity_search\", \"max_marginal_relevance\"] = \"similarity_search\"\n",
    "retrieval_meta: Literal[\"top3\", \"top5\", \"top10\"] = \"top5\"\n",
    "\n",
    "def get_answer_fn(question: str, history: Optional[List[Any]]=None) -> str:\n",
    "    \"\"\"A function representing your RAG agent.\"\"\"\n",
    "    # Format appropriately the history for your RAG agent\n",
    "    messages = history if history else []\n",
    "    if type(messages) == str:\n",
    "        raise ValueError(\"History should be a list of ChatMessage objects, not string.\")\n",
    "    messages.append({\"role\": \"user\", \"content\": question})\n",
    "\n",
    "\n",
    "    if retrieval_type==\"similarity_search\":\n",
    "        similarity_search_with_score = vector_store.similarity_search_with_score(question, k=5)\n",
    "        sources_str=\"\"\n",
    "        for i, doc_score in enumerate(similarity_search_with_score):\n",
    "            doc, score = doc_score\n",
    "            sources_str += f\"Source {i}. Relevency Score: {score}:\\n\"+ doc.page_content + \"\\n\\n\"\n",
    "    elif retrieval_type==\"max_marginal_relevance\":\n",
    "        docs = vector_store.max_marginal_relevance_search(question, k=5)\n",
    "        sources_str = \"\"\n",
    "        for i, doc in enumerate(docs):\n",
    "            sources_str += f\"Source {i}:\\n\"+ doc.page_content + \"\\n\\n\"\n",
    "\n",
    "    if prompt_type == \"api\":\n",
    "        system_message = \"Search results found the following information that might be relevent:\\n\\n\" \n",
    "\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message + sources_str})\n",
    "\n",
    "        reminder_message = \"Remember, you are on a phone call. Your response to the caller should be accurate and concise. Do not monologue. Here is the caller's message:\"\n",
    "        messages.append({\"role\": \"system\", \"content\": reminder_message})\n",
    "    elif prompt_type == \"p1\":\n",
    "        system_message = \"Search results found the following information that might be relevent:\\n\\n\" \n",
    "        messages.append({\"role\": \"system\", \"content\": system_message + sources_str})\n",
    "\n",
    "        instructions = \"You are helpful support agent who answers phone calls.\\n Search results will be given to you to help you answer questions. Only use those results to answer questions. If a topic comes up that you don't know, do not answer. You are to concisely answer their question, instead of quoting the information.Never insert additional information. If something is unclear, ask for clarification.\"\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": instructions})\n",
    "        messages.append({\"role\": \"system\", \"content\": instructions})\n",
    "    elif prompt_type == \"p2\":\n",
    "        system_message = \"Search results found the following information that might be relevent:\\n\\n\" \n",
    "        messages.append({\"role\": \"system\", \"content\": system_message + sources_str})\n",
    "\n",
    "        instructions = \"Remember, you are helpful support agent who answers phone calls.\\n Search results will be given to you to help you answer questions. Only use those results to answer questions. If a topic comes up that you don't know, do not answer. You are to concisely answer their question. If something is ambigious, ask for clarification.\"\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": instructions})\n",
    "        messages.append({\"role\": \"system\", \"content\": instructions})\n",
    "    chatcompletion = openai.chat.completions.create(messages=messages, model=\"gpt-4o-mini\")\n",
    "    answer = chatcompletion.choices[0].message.content\n",
    "    return answer\n",
    "\n",
    "\n",
    "#ragas_context_recall = RagasMetric(name=\"RAGAS Context Recall\", metric=context_recall, requires_context=True)\n",
    "\n",
    "from giskard.llm.client import LLMClient\n",
    "from openai.types.chat.completion_create_params import ResponseFormatJSONObject, ResponseFormatText\n",
    "class TesterLLM(LLMClient):\n",
    "    def complete(self, messages: Sequence[ChatMessage], temperature: float = 1, max_tokens: int | None = None, caller_id: str | None = None, seed: int | None = None, format=None) -> ChatMessage:\n",
    "        messages_openai = [{\"role\": message.role, \"content\": message.content} for message in messages]\n",
    "        if format is not None and \"json\" in format:\n",
    "            rformat: ResponseFormatJSONObject = {\"type\": \"json_object\"}\n",
    "        else:\n",
    "            rformat: ResponseFormatText = {\"type\": \"text\"}\n",
    "        response = openai.chat.completions.create(messages=messages_openai, model=\"gpt-4o-mini\", temperature=temperature, max_tokens=max_tokens, seed=seed, response_format=rformat)\n",
    "        \n",
    "        cm = ChatMessage(role=response.choices[0].message.role, content=response.choices[0].message.content)\n",
    "        return cm\n",
    "giskard.llm.set_default_client(TesterLLM())\n",
    "# giskard.llm.set_default_client(O1Client(model_openai=\"o1-mini\"))\n",
    "test_llm: Literal[None, \"gpt-4o-mini\", \"o1-mini\"] = \"gpt-4o-mini\"\n",
    "llm_client= TesterLLM()\n",
    "\n",
    "#test_llm: Literal[None, \"gpt-4o-mini\", \"o1-mini\"] = \"o1-mini\"\n",
    "#llm_client= O1Client(model_openai=\"o1-mini\")\n",
    "report = evaluate(get_answer_fn, testset=testset, knowledge_base=knowledge_base, llm_client=TesterLLM())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_llm: Literal[None, \"gpt-4o-mini\", \"o1-mini\"] = \"o1-mini\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(report)  # if you are working in a notebook\n",
    "\n",
    "# or save the report as an HTML file\n",
    "#report.to_html(\"rag_eval_report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_overlap = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.save(f\"reports_testset100-o1/{prompt_type}-gpt_4o_mini-splits_character_{chunk_size}_{chunk_overlap}-{retrieval_type}-{retrieval_meta}-zerox-tester_{test_llm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from giskard.rag import RAGReport\n",
    "loaded_report = RAGReport.load(\"reports/...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<giskard.rag.knowledge_base.KnowledgeBase at 0x341418ce0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report._knowledge_base._to_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>reference_context</th>\n",
       "      <th>conversation_history</th>\n",
       "      <th>metadata</th>\n",
       "      <th>agent_answer</th>\n",
       "      <th>correctness</th>\n",
       "      <th>correctness_reason</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9c63fed4-b956-4aa0-9501-741321f4f034</th>\n",
       "      <td>What happens if a student does not work throug...</td>\n",
       "      <td>If a student does not work through Labor Day, ...</td>\n",
       "      <td>Document 147: summer benefits\\nSummer housing ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'simple', 'seed_document_id'...</td>\n",
       "      <td>If a student living in summer housing does not...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent provided a general response about im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f29be739-0393-44a5-a519-7912dba4c9e1</th>\n",
       "      <td>What should a staff member do if they are goin...</td>\n",
       "      <td>They are expected to immediately call the Serv...</td>\n",
       "      <td>Document 45: If a staff member is unable to re...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'simple', 'seed_document_id'...</td>\n",
       "      <td>If a staff member knows they will be late for ...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent provided a general response about no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fb530e2e-e211-41f1-9c11-3e5bc8078666</th>\n",
       "      <td>In order to be considered for a promotion to t...</td>\n",
       "      <td>An employee must work a minimum of 120 total h...</td>\n",
       "      <td>Document 135: a. Possess a satisfactory perfor...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'complex', 'seed_document_id...</td>\n",
       "      <td>The minimum total number of hours an employee ...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent did not provide the specific minimum...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c9e0790a-3be2-4adf-b9bd-7de3f0c01105</th>\n",
       "      <td>Could you specify the individuals who hold the...</td>\n",
       "      <td>Only the Service Center Assistant Manager, Man...</td>\n",
       "      <td>Document 162: All keys, swipes and key rings i...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'complex', 'seed_document_id...</td>\n",
       "      <td>The authority to approve the signing out of ke...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent provided a broader list of roles tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94e282d7-352b-42f9-b78e-762e24a61c1d</th>\n",
       "      <td>What steps should a student take regarding the...</td>\n",
       "      <td>If you have a student parking permit, remove i...</td>\n",
       "      <td>Document 55: If you have a student parking per...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'distracting element', 'seed...</td>\n",
       "      <td>If a student has received a written warning fo...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent provided a detailed response about g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e049d396-2658-4b2f-8722-e42697cdc731</th>\n",
       "      <td>In the event that a substitute fails to report...</td>\n",
       "      <td>The original (scheduled/assigned) staff member...</td>\n",
       "      <td>Document 48: If the substitute does not report...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'distracting element', 'seed...</td>\n",
       "      <td>When a substitute fails to report for their sc...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent provided a detailed response about t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14060525-e7cd-4fa3-b239-501a78dc994c</th>\n",
       "      <td>Hi, I'm a staff member at the university and I...</td>\n",
       "      <td>The driver should take the vehicle out to Moto...</td>\n",
       "      <td>Document 171: Vehicles should never be returne...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'situational', 'seed_documen...</td>\n",
       "      <td>It's important to ensure that university vehic...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent provided general refueling procedure...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42d9bf93-4e26-4fef-b301-c31e7cd69bd8</th>\n",
       "      <td>Hi there, I’m currently dealing with an unexpe...</td>\n",
       "      <td>If a staff member is not able to safely travel...</td>\n",
       "      <td>Document 63: In the event the University has a...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'situational', 'seed_documen...</td>\n",
       "      <td>If you are a staff member unable to safely tra...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent provided a detailed response about g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ad2c7d12-7bb1-4710-9efc-16f83d2bbbeb</th>\n",
       "      <td>What is the main purpose of the Service Center...</td>\n",
       "      <td>The Service Center serves as the main communic...</td>\n",
       "      <td>Document 6: The Department of Residential Faci...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'double', 'original_question...</td>\n",
       "      <td>The main purpose of the Service Center is to p...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent provided a general description of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dc21b28c-3d62-4a84-83fc-ba8b83599849</th>\n",
       "      <td>What are the consequences?</td>\n",
       "      <td>Forgetting ID when working and using key slip ...</td>\n",
       "      <td>Document 182: not completing work in the box o...</td>\n",
       "      <td>[{'role': 'user', 'content': 'I want to know w...</td>\n",
       "      <td>{'question_type': 'conversational', 'seed_docu...</td>\n",
       "      <td>The consequences of an employee forgetting the...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent provided a detailed explanation of p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               question  \\\n",
       "id                                                                                        \n",
       "9c63fed4-b956-4aa0-9501-741321f4f034  What happens if a student does not work throug...   \n",
       "f29be739-0393-44a5-a519-7912dba4c9e1  What should a staff member do if they are goin...   \n",
       "fb530e2e-e211-41f1-9c11-3e5bc8078666  In order to be considered for a promotion to t...   \n",
       "c9e0790a-3be2-4adf-b9bd-7de3f0c01105  Could you specify the individuals who hold the...   \n",
       "94e282d7-352b-42f9-b78e-762e24a61c1d  What steps should a student take regarding the...   \n",
       "e049d396-2658-4b2f-8722-e42697cdc731  In the event that a substitute fails to report...   \n",
       "14060525-e7cd-4fa3-b239-501a78dc994c  Hi, I'm a staff member at the university and I...   \n",
       "42d9bf93-4e26-4fef-b301-c31e7cd69bd8  Hi there, I’m currently dealing with an unexpe...   \n",
       "ad2c7d12-7bb1-4710-9efc-16f83d2bbbeb  What is the main purpose of the Service Center...   \n",
       "dc21b28c-3d62-4a84-83fc-ba8b83599849                         What are the consequences?   \n",
       "\n",
       "                                                                       reference_answer  \\\n",
       "id                                                                                        \n",
       "9c63fed4-b956-4aa0-9501-741321f4f034  If a student does not work through Labor Day, ...   \n",
       "f29be739-0393-44a5-a519-7912dba4c9e1  They are expected to immediately call the Serv...   \n",
       "fb530e2e-e211-41f1-9c11-3e5bc8078666  An employee must work a minimum of 120 total h...   \n",
       "c9e0790a-3be2-4adf-b9bd-7de3f0c01105  Only the Service Center Assistant Manager, Man...   \n",
       "94e282d7-352b-42f9-b78e-762e24a61c1d  If you have a student parking permit, remove i...   \n",
       "e049d396-2658-4b2f-8722-e42697cdc731  The original (scheduled/assigned) staff member...   \n",
       "14060525-e7cd-4fa3-b239-501a78dc994c  The driver should take the vehicle out to Moto...   \n",
       "42d9bf93-4e26-4fef-b301-c31e7cd69bd8  If a staff member is not able to safely travel...   \n",
       "ad2c7d12-7bb1-4710-9efc-16f83d2bbbeb  The Service Center serves as the main communic...   \n",
       "dc21b28c-3d62-4a84-83fc-ba8b83599849  Forgetting ID when working and using key slip ...   \n",
       "\n",
       "                                                                      reference_context  \\\n",
       "id                                                                                        \n",
       "9c63fed4-b956-4aa0-9501-741321f4f034  Document 147: summer benefits\\nSummer housing ...   \n",
       "f29be739-0393-44a5-a519-7912dba4c9e1  Document 45: If a staff member is unable to re...   \n",
       "fb530e2e-e211-41f1-9c11-3e5bc8078666  Document 135: a. Possess a satisfactory perfor...   \n",
       "c9e0790a-3be2-4adf-b9bd-7de3f0c01105  Document 162: All keys, swipes and key rings i...   \n",
       "94e282d7-352b-42f9-b78e-762e24a61c1d  Document 55: If you have a student parking per...   \n",
       "e049d396-2658-4b2f-8722-e42697cdc731  Document 48: If the substitute does not report...   \n",
       "14060525-e7cd-4fa3-b239-501a78dc994c  Document 171: Vehicles should never be returne...   \n",
       "42d9bf93-4e26-4fef-b301-c31e7cd69bd8  Document 63: In the event the University has a...   \n",
       "ad2c7d12-7bb1-4710-9efc-16f83d2bbbeb  Document 6: The Department of Residential Faci...   \n",
       "dc21b28c-3d62-4a84-83fc-ba8b83599849  Document 182: not completing work in the box o...   \n",
       "\n",
       "                                                                   conversation_history  \\\n",
       "id                                                                                        \n",
       "9c63fed4-b956-4aa0-9501-741321f4f034                                                 []   \n",
       "f29be739-0393-44a5-a519-7912dba4c9e1                                                 []   \n",
       "fb530e2e-e211-41f1-9c11-3e5bc8078666                                                 []   \n",
       "c9e0790a-3be2-4adf-b9bd-7de3f0c01105                                                 []   \n",
       "94e282d7-352b-42f9-b78e-762e24a61c1d                                                 []   \n",
       "e049d396-2658-4b2f-8722-e42697cdc731                                                 []   \n",
       "14060525-e7cd-4fa3-b239-501a78dc994c                                                 []   \n",
       "42d9bf93-4e26-4fef-b301-c31e7cd69bd8                                                 []   \n",
       "ad2c7d12-7bb1-4710-9efc-16f83d2bbbeb                                                 []   \n",
       "dc21b28c-3d62-4a84-83fc-ba8b83599849  [{'role': 'user', 'content': 'I want to know w...   \n",
       "\n",
       "                                                                               metadata  \\\n",
       "id                                                                                        \n",
       "9c63fed4-b956-4aa0-9501-741321f4f034  {'question_type': 'simple', 'seed_document_id'...   \n",
       "f29be739-0393-44a5-a519-7912dba4c9e1  {'question_type': 'simple', 'seed_document_id'...   \n",
       "fb530e2e-e211-41f1-9c11-3e5bc8078666  {'question_type': 'complex', 'seed_document_id...   \n",
       "c9e0790a-3be2-4adf-b9bd-7de3f0c01105  {'question_type': 'complex', 'seed_document_id...   \n",
       "94e282d7-352b-42f9-b78e-762e24a61c1d  {'question_type': 'distracting element', 'seed...   \n",
       "e049d396-2658-4b2f-8722-e42697cdc731  {'question_type': 'distracting element', 'seed...   \n",
       "14060525-e7cd-4fa3-b239-501a78dc994c  {'question_type': 'situational', 'seed_documen...   \n",
       "42d9bf93-4e26-4fef-b301-c31e7cd69bd8  {'question_type': 'situational', 'seed_documen...   \n",
       "ad2c7d12-7bb1-4710-9efc-16f83d2bbbeb  {'question_type': 'double', 'original_question...   \n",
       "dc21b28c-3d62-4a84-83fc-ba8b83599849  {'question_type': 'conversational', 'seed_docu...   \n",
       "\n",
       "                                                                           agent_answer  \\\n",
       "id                                                                                        \n",
       "9c63fed4-b956-4aa0-9501-741321f4f034  If a student living in summer housing does not...   \n",
       "f29be739-0393-44a5-a519-7912dba4c9e1  If a staff member knows they will be late for ...   \n",
       "fb530e2e-e211-41f1-9c11-3e5bc8078666  The minimum total number of hours an employee ...   \n",
       "c9e0790a-3be2-4adf-b9bd-7de3f0c01105  The authority to approve the signing out of ke...   \n",
       "94e282d7-352b-42f9-b78e-762e24a61c1d  If a student has received a written warning fo...   \n",
       "e049d396-2658-4b2f-8722-e42697cdc731  When a substitute fails to report for their sc...   \n",
       "14060525-e7cd-4fa3-b239-501a78dc994c  It's important to ensure that university vehic...   \n",
       "42d9bf93-4e26-4fef-b301-c31e7cd69bd8  If you are a staff member unable to safely tra...   \n",
       "ad2c7d12-7bb1-4710-9efc-16f83d2bbbeb  The main purpose of the Service Center is to p...   \n",
       "dc21b28c-3d62-4a84-83fc-ba8b83599849  The consequences of an employee forgetting the...   \n",
       "\n",
       "                                      correctness  \\\n",
       "id                                                  \n",
       "9c63fed4-b956-4aa0-9501-741321f4f034        False   \n",
       "f29be739-0393-44a5-a519-7912dba4c9e1        False   \n",
       "fb530e2e-e211-41f1-9c11-3e5bc8078666        False   \n",
       "c9e0790a-3be2-4adf-b9bd-7de3f0c01105        False   \n",
       "94e282d7-352b-42f9-b78e-762e24a61c1d        False   \n",
       "e049d396-2658-4b2f-8722-e42697cdc731        False   \n",
       "14060525-e7cd-4fa3-b239-501a78dc994c        False   \n",
       "42d9bf93-4e26-4fef-b301-c31e7cd69bd8        False   \n",
       "ad2c7d12-7bb1-4710-9efc-16f83d2bbbeb        False   \n",
       "dc21b28c-3d62-4a84-83fc-ba8b83599849        False   \n",
       "\n",
       "                                                                     correctness_reason  \n",
       "id                                                                                       \n",
       "9c63fed4-b956-4aa0-9501-741321f4f034  The agent provided a general response about im...  \n",
       "f29be739-0393-44a5-a519-7912dba4c9e1  The agent provided a general response about no...  \n",
       "fb530e2e-e211-41f1-9c11-3e5bc8078666  The agent did not provide the specific minimum...  \n",
       "c9e0790a-3be2-4adf-b9bd-7de3f0c01105  The agent provided a broader list of roles tha...  \n",
       "94e282d7-352b-42f9-b78e-762e24a61c1d  The agent provided a detailed response about g...  \n",
       "e049d396-2658-4b2f-8722-e42697cdc731  The agent provided a detailed response about t...  \n",
       "14060525-e7cd-4fa3-b239-501a78dc994c  The agent provided general refueling procedure...  \n",
       "42d9bf93-4e26-4fef-b301-c31e7cd69bd8  The agent provided a detailed response about g...  \n",
       "ad2c7d12-7bb1-4710-9efc-16f83d2bbbeb  The agent provided a general description of th...  \n",
       "dc21b28c-3d62-4a84-83fc-ba8b83599849  The agent provided a detailed explanation of p...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone.grpc import PineconeGRPC as Pinecone\n",
    "from pinecone.grpc import GRPCVector\n",
    "from pinecone.grpc.index_grpc import UpsertResponse\n",
    "\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_HOST = os.getenv(\"PINECONE_HOST\")\n",
    "PINECONE_NAMESPACE = \"umd-call-center\"\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "PC_INDEX_NAME = \"knowledge\"\n",
    "pc_index = pc.Index(PC_INDEX_NAME, host=PINECONE_HOST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "def query_pc(vector: List[float]):\n",
    "    query_result = pc_index.query(\n",
    "        vector=vector,\n",
    "        namespace=PINECONE_NAMESPACE,\n",
    "        top_k=10,\n",
    "        #filter={\"knowledge_uuid\": {\"$in\": knowledge_uuids}},\n",
    "        include_metadata=True,\n",
    "        timeout=1,\n",
    "    )\n",
    "    return query_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    # Compute dot product\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    # Compute magnitudes\n",
    "    magnitude_vec1 = np.linalg.norm(vec1)\n",
    "    magnitude_vec2 = np.linalg.norm(vec2)\n",
    "    # Calculate cosine similarity\n",
    "    if magnitude_vec1 == 0 or magnitude_vec2 == 0:\n",
    "        return 0  # Handle the case of zero magnitude\n",
    "    return dot_product / (magnitude_vec1 * magnitude_vec2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
